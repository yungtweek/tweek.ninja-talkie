import asyncio
from logging import getLogger
from langchain_core.callbacks import AsyncCallbackHandler
from chat_worker.infrastructure.stream.stream_service import safe_publish

log = getLogger(__name__)


class TokenStreamCallback(AsyncCallbackHandler):
    """
    LangChain Callback: Called each time a new token is generated by the LLM.
    - `publish(evt)` is injected externally (e.g., a Redis publish wrapper).
    - Emitted events use keys: event, jobId, userId, index, content (for chunks), and done/error events.
    """

    def __init__(self, job_id: str, user_id: str, publish, aggregate_final: bool = True, allowed_tags: set[str] | None = None):
        self.publish = publish
        self.job_id = job_id
        self.user_id = user_id
        self.aggregate_final = aggregate_final
        self.allowed_tags = allowed_tags
        self._buf: list[str] = []
        self._ended = False
        self._i = 0  # chunk sequence index
        # ðŸ”‘ Capture the main event loop for thread-safe scheduling
        self._loop = asyncio.get_running_loop()

        # Ensure safe cross-thread invocation from llama.cpp callbacks


    def on_llm_new_token(self, token: str, **kwargs):
        tags = kwargs.get("tags") or []
        if self.allowed_tags and not any(t in self.allowed_tags for t in tags):
            return
        # Optionally buffer tokens to reconstruct the final text later
        if self.aggregate_final and token:
            self._buf.append(token)
        # Never await directly in a non-async thread; schedule safely on the main loop
        asyncio.run_coroutine_threadsafe(
            safe_publish(self.publish,
                         {"event": "token", "jobId": self.job_id, "userId": self.user_id, "index": self._i,
                          "content": token}),
            self._loop,
        )
        self._i += 1

    def on_llm_end(self, response, **kwargs):
        tags = kwargs.get("tags") or []
        if self.allowed_tags and not any(t in self.allowed_tags for t in tags):
            return
        if self._ended:
            return
        self._ended = True
        asyncio.run_coroutine_threadsafe(
            safe_publish(self.publish, {"event": "done", "jobId": self.job_id, "userId": self.user_id}, ),
            self._loop,
        )

    def on_llm_error(self, error: BaseException, **kwargs):
        if self._ended:
            return
        self._ended = True
        asyncio.run_coroutine_threadsafe(
            safe_publish(self.publish, {
                "event": "error", "jobId": self.job_id, "userId": self.user_id,
                "code": "LLM_ERROR", "message": str(error), "retryable": False
            }),
            self._loop,
        )

    def final_text(self) -> str:
        """Return the concatenated string of all accumulated tokens."""
        if not self._buf:
            return ""
        return "".join(self._buf)
