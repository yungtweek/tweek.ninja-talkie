// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.10
// 	protoc        v5.29.3
// source: llm.proto

package llm

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// 공통 메타데이터 (로그/추적용)
type LlmMetadata struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	RequestId     string                 `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"` // 워커 쪽에서 생성하는 요청 ID
	TraceId       string                 `protobuf:"bytes,2,opt,name=trace_id,json=traceId,proto3" json:"trace_id,omitempty"`       // 분산 트레이싱용 (선택)
	SessionId     string                 `protobuf:"bytes,3,opt,name=session_id,json=sessionId,proto3" json:"session_id,omitempty"` // 유저 세션 / 대화 ID
	UserId        string                 `protobuf:"bytes,4,opt,name=user_id,json=userId,proto3" json:"user_id,omitempty"`          // 유저 식별자 (익명 ID 가능)
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *LlmMetadata) Reset() {
	*x = LlmMetadata{}
	mi := &file_llm_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LlmMetadata) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LlmMetadata) ProtoMessage() {}

func (x *LlmMetadata) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LlmMetadata.ProtoReflect.Descriptor instead.
func (*LlmMetadata) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{0}
}

func (x *LlmMetadata) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *LlmMetadata) GetTraceId() string {
	if x != nil {
		return x.TraceId
	}
	return ""
}

func (x *LlmMetadata) GetSessionId() string {
	if x != nil {
		return x.SessionId
	}
	return ""
}

func (x *LlmMetadata) GetUserId() string {
	if x != nil {
		return x.UserId
	}
	return ""
}

// ChatCompletion 요청
type ChatCompletionRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Meta          *LlmMetadata           `protobuf:"bytes,1,opt,name=meta,proto3" json:"meta,omitempty"`
	Model         string                 `protobuf:"bytes,2,opt,name=model,proto3" json:"model,omitempty"`                                   // 예: "qwen2.5-7b-instruct"
	SystemPrompt  string                 `protobuf:"bytes,3,opt,name=system_prompt,json=systemPrompt,proto3" json:"system_prompt,omitempty"` // 시스템 역할 프롬프트
	UserPrompt    string                 `protobuf:"bytes,4,opt,name=user_prompt,json=userPrompt,proto3" json:"user_prompt,omitempty"`       // 유저 실제 질문
	Context       string                 `protobuf:"bytes,5,opt,name=context,proto3" json:"context,omitempty"`                               // RAG 컨텍스트 텍스트 (없으면 빈 문자열)
	Temperature   float64                `protobuf:"fixed64,6,opt,name=temperature,proto3" json:"temperature,omitempty"`                     // 0.0 ~ 2.0
	MaxTokens     int32                  `protobuf:"varint,7,opt,name=max_tokens,json=maxTokens,proto3" json:"max_tokens,omitempty"`         // 생성 최대 토큰 수
	TopP          float64                `protobuf:"fixed64,8,opt,name=top_p,json=topP,proto3" json:"top_p,omitempty"`                       // nucleus sampling
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ChatCompletionRequest) Reset() {
	*x = ChatCompletionRequest{}
	mi := &file_llm_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatCompletionRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatCompletionRequest) ProtoMessage() {}

func (x *ChatCompletionRequest) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatCompletionRequest.ProtoReflect.Descriptor instead.
func (*ChatCompletionRequest) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{1}
}

func (x *ChatCompletionRequest) GetMeta() *LlmMetadata {
	if x != nil {
		return x.Meta
	}
	return nil
}

func (x *ChatCompletionRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *ChatCompletionRequest) GetSystemPrompt() string {
	if x != nil {
		return x.SystemPrompt
	}
	return ""
}

func (x *ChatCompletionRequest) GetUserPrompt() string {
	if x != nil {
		return x.UserPrompt
	}
	return ""
}

func (x *ChatCompletionRequest) GetContext() string {
	if x != nil {
		return x.Context
	}
	return ""
}

func (x *ChatCompletionRequest) GetTemperature() float64 {
	if x != nil {
		return x.Temperature
	}
	return 0
}

func (x *ChatCompletionRequest) GetMaxTokens() int32 {
	if x != nil {
		return x.MaxTokens
	}
	return 0
}

func (x *ChatCompletionRequest) GetTopP() float64 {
	if x != nil {
		return x.TopP
	}
	return 0
}

// ChatCompletion 응답
type ChatCompletionResponse struct {
	state            protoimpl.MessageState `protogen:"open.v1"`
	OutputText       string                 `protobuf:"bytes,1,opt,name=output_text,json=outputText,proto3" json:"output_text,omitempty"`       // 최종 생성 텍스트
	FinishReason     string                 `protobuf:"bytes,2,opt,name=finish_reason,json=finishReason,proto3" json:"finish_reason,omitempty"` // 예: "stop", "length", "error" 등
	PromptTokens     int32                  `protobuf:"varint,3,opt,name=prompt_tokens,json=promptTokens,proto3" json:"prompt_tokens,omitempty"`
	CompletionTokens int32                  `protobuf:"varint,4,opt,name=completion_tokens,json=completionTokens,proto3" json:"completion_tokens,omitempty"`
	TotalTokens      int32                  `protobuf:"varint,5,opt,name=total_tokens,json=totalTokens,proto3" json:"total_tokens,omitempty"`
	LatencyMs        int64                  `protobuf:"varint,6,opt,name=latency_ms,json=latencyMs,proto3" json:"latency_ms,omitempty"` // gateway 기준 end-start ms
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *ChatCompletionResponse) Reset() {
	*x = ChatCompletionResponse{}
	mi := &file_llm_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatCompletionResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatCompletionResponse) ProtoMessage() {}

func (x *ChatCompletionResponse) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatCompletionResponse.ProtoReflect.Descriptor instead.
func (*ChatCompletionResponse) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{2}
}

func (x *ChatCompletionResponse) GetOutputText() string {
	if x != nil {
		return x.OutputText
	}
	return ""
}

func (x *ChatCompletionResponse) GetFinishReason() string {
	if x != nil {
		return x.FinishReason
	}
	return ""
}

func (x *ChatCompletionResponse) GetPromptTokens() int32 {
	if x != nil {
		return x.PromptTokens
	}
	return 0
}

func (x *ChatCompletionResponse) GetCompletionTokens() int32 {
	if x != nil {
		return x.CompletionTokens
	}
	return 0
}

func (x *ChatCompletionResponse) GetTotalTokens() int32 {
	if x != nil {
		return x.TotalTokens
	}
	return 0
}

func (x *ChatCompletionResponse) GetLatencyMs() int64 {
	if x != nil {
		return x.LatencyMs
	}
	return 0
}

type ChatCompletionChunkResponse struct {
	state            protoimpl.MessageState `protogen:"open.v1"`
	DeltaText        string                 `protobuf:"bytes,1,opt,name=deltaText,proto3" json:"deltaText,omitempty"`        // 이번 청크에서 추가된 텍스트
	FinishReason     string                 `protobuf:"bytes,2,opt,name=finishReason,proto3" json:"finishReason,omitempty"`  // 마지막 청크에서만 세팅
	Index            int32                  `protobuf:"varint,3,opt,name=index,proto3" json:"index,omitempty"`               // choice index, 보통 0
	PromptTokens     int32                  `protobuf:"varint,4,opt,name=promptTokens,proto3" json:"promptTokens,omitempty"` // 마지막 청크에서만 세팅
	CompletionTokens int32                  `protobuf:"varint,5,opt,name=completionTokens,proto3" json:"completionTokens,omitempty"`
	TotalTokens      int32                  `protobuf:"varint,6,opt,name=totalTokens,proto3" json:"totalTokens,omitempty"`
	LatencyMs        int64                  `protobuf:"varint,7,opt,name=latencyMs,proto3" json:"latencyMs,omitempty"` // 옵션: 첫 토큰 또는 전체
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *ChatCompletionChunkResponse) Reset() {
	*x = ChatCompletionChunkResponse{}
	mi := &file_llm_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatCompletionChunkResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatCompletionChunkResponse) ProtoMessage() {}

func (x *ChatCompletionChunkResponse) ProtoReflect() protoreflect.Message {
	mi := &file_llm_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatCompletionChunkResponse.ProtoReflect.Descriptor instead.
func (*ChatCompletionChunkResponse) Descriptor() ([]byte, []int) {
	return file_llm_proto_rawDescGZIP(), []int{3}
}

func (x *ChatCompletionChunkResponse) GetDeltaText() string {
	if x != nil {
		return x.DeltaText
	}
	return ""
}

func (x *ChatCompletionChunkResponse) GetFinishReason() string {
	if x != nil {
		return x.FinishReason
	}
	return ""
}

func (x *ChatCompletionChunkResponse) GetIndex() int32 {
	if x != nil {
		return x.Index
	}
	return 0
}

func (x *ChatCompletionChunkResponse) GetPromptTokens() int32 {
	if x != nil {
		return x.PromptTokens
	}
	return 0
}

func (x *ChatCompletionChunkResponse) GetCompletionTokens() int32 {
	if x != nil {
		return x.CompletionTokens
	}
	return 0
}

func (x *ChatCompletionChunkResponse) GetTotalTokens() int32 {
	if x != nil {
		return x.TotalTokens
	}
	return 0
}

func (x *ChatCompletionChunkResponse) GetLatencyMs() int64 {
	if x != nil {
		return x.LatencyMs
	}
	return 0
}

var File_llm_proto protoreflect.FileDescriptor

const file_llm_proto_rawDesc = "" +
	"\n" +
	"\tllm.proto\x12\x06llm.v1\"\x7f\n" +
	"\vLlmMetadata\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12\x19\n" +
	"\btrace_id\x18\x02 \x01(\tR\atraceId\x12\x1d\n" +
	"\n" +
	"session_id\x18\x03 \x01(\tR\tsessionId\x12\x17\n" +
	"\auser_id\x18\x04 \x01(\tR\x06userId\"\x8c\x02\n" +
	"\x15ChatCompletionRequest\x12'\n" +
	"\x04meta\x18\x01 \x01(\v2\x13.llm.v1.LlmMetadataR\x04meta\x12\x14\n" +
	"\x05model\x18\x02 \x01(\tR\x05model\x12#\n" +
	"\rsystem_prompt\x18\x03 \x01(\tR\fsystemPrompt\x12\x1f\n" +
	"\vuser_prompt\x18\x04 \x01(\tR\n" +
	"userPrompt\x12\x18\n" +
	"\acontext\x18\x05 \x01(\tR\acontext\x12 \n" +
	"\vtemperature\x18\x06 \x01(\x01R\vtemperature\x12\x1d\n" +
	"\n" +
	"max_tokens\x18\a \x01(\x05R\tmaxTokens\x12\x13\n" +
	"\x05top_p\x18\b \x01(\x01R\x04topP\"\xf2\x01\n" +
	"\x16ChatCompletionResponse\x12\x1f\n" +
	"\voutput_text\x18\x01 \x01(\tR\n" +
	"outputText\x12#\n" +
	"\rfinish_reason\x18\x02 \x01(\tR\ffinishReason\x12#\n" +
	"\rprompt_tokens\x18\x03 \x01(\x05R\fpromptTokens\x12+\n" +
	"\x11completion_tokens\x18\x04 \x01(\x05R\x10completionTokens\x12!\n" +
	"\ftotal_tokens\x18\x05 \x01(\x05R\vtotalTokens\x12\x1d\n" +
	"\n" +
	"latency_ms\x18\x06 \x01(\x03R\tlatencyMs\"\x85\x02\n" +
	"\x1bChatCompletionChunkResponse\x12\x1c\n" +
	"\tdeltaText\x18\x01 \x01(\tR\tdeltaText\x12\"\n" +
	"\ffinishReason\x18\x02 \x01(\tR\ffinishReason\x12\x14\n" +
	"\x05index\x18\x03 \x01(\x05R\x05index\x12\"\n" +
	"\fpromptTokens\x18\x04 \x01(\x05R\fpromptTokens\x12*\n" +
	"\x10completionTokens\x18\x05 \x01(\x05R\x10completionTokens\x12 \n" +
	"\vtotalTokens\x18\x06 \x01(\x05R\vtotalTokens\x12\x1c\n" +
	"\tlatencyMs\x18\a \x01(\x03R\tlatencyMs2\xbb\x01\n" +
	"\n" +
	"LlmService\x12O\n" +
	"\x0eChatCompletion\x12\x1d.llm.v1.ChatCompletionRequest\x1a\x1e.llm.v1.ChatCompletionResponse\x12\\\n" +
	"\x14ChatCompletionStream\x12\x1d.llm.v1.ChatCompletionRequest\x1a#.llm.v1.ChatCompletionChunkResponse0\x01B:Z8github.com/yungtweek/talkie/apps/llm-gateway/gen/llm;llmb\x06proto3"

var (
	file_llm_proto_rawDescOnce sync.Once
	file_llm_proto_rawDescData []byte
)

func file_llm_proto_rawDescGZIP() []byte {
	file_llm_proto_rawDescOnce.Do(func() {
		file_llm_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_llm_proto_rawDesc), len(file_llm_proto_rawDesc)))
	})
	return file_llm_proto_rawDescData
}

var file_llm_proto_msgTypes = make([]protoimpl.MessageInfo, 4)
var file_llm_proto_goTypes = []any{
	(*LlmMetadata)(nil),                 // 0: llm.v1.LlmMetadata
	(*ChatCompletionRequest)(nil),       // 1: llm.v1.ChatCompletionRequest
	(*ChatCompletionResponse)(nil),      // 2: llm.v1.ChatCompletionResponse
	(*ChatCompletionChunkResponse)(nil), // 3: llm.v1.ChatCompletionChunkResponse
}
var file_llm_proto_depIdxs = []int32{
	0, // 0: llm.v1.ChatCompletionRequest.meta:type_name -> llm.v1.LlmMetadata
	1, // 1: llm.v1.LlmService.ChatCompletion:input_type -> llm.v1.ChatCompletionRequest
	1, // 2: llm.v1.LlmService.ChatCompletionStream:input_type -> llm.v1.ChatCompletionRequest
	2, // 3: llm.v1.LlmService.ChatCompletion:output_type -> llm.v1.ChatCompletionResponse
	3, // 4: llm.v1.LlmService.ChatCompletionStream:output_type -> llm.v1.ChatCompletionChunkResponse
	3, // [3:5] is the sub-list for method output_type
	1, // [1:3] is the sub-list for method input_type
	1, // [1:1] is the sub-list for extension type_name
	1, // [1:1] is the sub-list for extension extendee
	0, // [0:1] is the sub-list for field type_name
}

func init() { file_llm_proto_init() }
func file_llm_proto_init() {
	if File_llm_proto != nil {
		return
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_llm_proto_rawDesc), len(file_llm_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   4,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_llm_proto_goTypes,
		DependencyIndexes: file_llm_proto_depIdxs,
		MessageInfos:      file_llm_proto_msgTypes,
	}.Build()
	File_llm_proto = out.File
	file_llm_proto_goTypes = nil
	file_llm_proto_depIdxs = nil
}
